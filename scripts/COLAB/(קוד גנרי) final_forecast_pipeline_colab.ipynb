{"cells":[{"cell_type":"code","source":["!pip install tensorflow\n"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"NFIJ2fO9DFHC","executionInfo":{"status":"ok","timestamp":1753739988909,"user_tz":-180,"elapsed":13494,"user":{"displayName":"Ariel.Kariv1","userId":"11324313568200724952"}},"outputId":"53f157a0-e528-44b3-8b45-56bb305cd0e2"},"id":"NFIJ2fO9DFHC","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"]}]},{"cell_type":"code","execution_count":null,"id":"af879de8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"af879de8","executionInfo":{"status":"ok","timestamp":1753740516617,"user_tz":-180,"elapsed":2170,"user":{"displayName":"Ariel.Kariv1","userId":"11324313568200724952"}},"outputId":"cf1bba6e-fbd4-45b5-a842-3ddc419c0ede"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"0b739c83","metadata":{"id":"0b739c83"},"outputs":[],"source":["import os\n","\n","BASE_DIR = \"/content/drive/MyDrive/Final_Project\"\n","RAW_DIR = os.path.join(BASE_DIR, \"data\", \"raw\")\n","PROCESSED_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\n","MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n","OUTPUT_JSON = os.path.join(BASE_DIR, \"outputs\", \"json\")\n","OUTPUT_PLOTS = os.path.join(BASE_DIR, \"outputs\", \"plots\")\n","\n","# ×™×¦×™×¨×ª ×ª×™×§×™×•×ª ×× ×œ× ×§×™×™××•×ª\n","for path in [PROCESSED_DIR, MODELS_DIR, OUTPUT_JSON, OUTPUT_PLOTS]:\n","    os.makedirs(path, exist_ok=True)\n"]},{"cell_type":"markdown","source":["### ×™×¦×™×¨×ª ×˜×‘×œ×ª ×¤×™×¦'×¨×™× ××œ××” (S&P + ×—×“×©×•×ª ×›×œ×›×œ×™×•×ª)"],"metadata":{"id":"wMpLerCdNMhY"},"id":"wMpLerCdNMhY"},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","\n","# × ×ª×™×‘×™×\n","BASE_DIR = \"/content/drive/MyDrive/Final_Project\"\n","RAW_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\n","PROCESSED_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\n","os.makedirs(PROCESSED_DIR, exist_ok=True)\n","\n","snp_path = os.path.join(RAW_DIR, \"SNP_DATA.csv\")\n","news_path = os.path.join(RAW_DIR, \"NEW_DATA_××•×’××¨.csv\")\n","output_path = os.path.join(PROCESSED_DIR, \"SNP_NEWS_FINAL.csv\")\n","\n","# --- ×©×œ×‘ 1: ×§×¨×™××ª × ×ª×•× ×™× ---\n","snp_df = pd.read_csv(snp_path, low_memory=False, on_bad_lines='skip')\n","news_df = pd.read_csv(news_path, low_memory=False)\n","\n","# --- ×©×œ×‘ 2: ×˜×™×¤×•×¡ ×ª××¨×™×š ---\n","news_df['Date'] = pd.to_datetime(news_df['Date'], dayfirst=True, errors='coerce')\n","news_df = news_df[news_df['Date'].notna()]\n","snp_df['Date'] = pd.to_datetime(snp_df['Date'], dayfirst=True, errors='coerce')\n","snp_df = snp_df[snp_df['Date'].notna()].copy()\n","\n","# --- ×©×œ×‘ 3: ×™×¦×™×¨×ª ×¤×™×¦'×¨×™× ×œ××“×“ ---\n","required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n","for col in required_cols:\n","    snp_df[col] = pd.to_numeric(snp_df[col], errors='coerce')\n","\n","snp_df['Pct_Change'] = snp_df['Close'].pct_change() * 100\n","snp_df['MA_5'] = snp_df['Close'].rolling(window=5).mean()\n","snp_df['STD_5'] = snp_df['Close'].rolling(window=5).std()\n","\n","delta = snp_df['Close'].diff()\n","gain = delta.clip(lower=0)\n","loss = -delta.clip(upper=0)\n","avg_gain = gain.rolling(window=14).mean()\n","avg_loss = loss.rolling(window=14).mean()\n","rs = avg_gain / avg_loss\n","snp_df['RSI'] = 100 - (100 / (1 + rs))\n","\n","ema_12 = snp_df['Close'].ewm(span=12, adjust=False).mean()\n","ema_26 = snp_df['Close'].ewm(span=26, adjust=False).mean()\n","snp_df['MACD'] = ema_12 - ema_26\n","snp_df['MACD_RSI_Ratio'] = snp_df['MACD'] / snp_df['RSI']\n","snp_df['Change_Pct'] = ((snp_df['Close'] - snp_df['Open']) / snp_df['Open']) * 100\n","snp_df['Volatility'] = snp_df[['Open', 'High', 'Low', 'Close']].std(axis=1)\n","\n","# --- ×©×œ×‘ 4: ×¡× ×˜×™×× ×˜ ×™×•××™ ---\n","sentiment_map = {'bullish': 1, 'bearish': -1, 'neutral': 0}\n","news_df['overall_sentiment_label'] = news_df['overall_sentiment_label'].astype(str).str.lower().map(sentiment_map)\n","sentiment_col = [col for col in news_df.columns if 'overall_sentiment_score' in col][0]\n","news_df[sentiment_col] = pd.to_numeric(news_df[sentiment_col], errors='coerce')\n","\n","daily_sentiment = news_df.groupby('Date')[sentiment_col].mean().reset_index()\n","daily_sentiment.rename(columns={sentiment_col: 'Sentiment_Score'}, inplace=True)\n","daily_label = news_df.groupby('Date')['overall_sentiment_label'].mean().reset_index()\n","daily_label.rename(columns={'overall_sentiment_label': 'sentiment_label'}, inplace=True)\n","\n","# --- ×©×œ×‘ 5: ××™×–×•×’ ×•×™×¦×™×¨×ª ×¤×™×¦'×¨×™× × ×•×¡×¤×™× ---\n","merged_df = pd.merge(snp_df, daily_sentiment, on=\"Date\", how=\"left\")\n","merged_df = pd.merge(merged_df, daily_label, on=\"Date\", how=\"left\")\n","merged_df['Is_News_Day'] = merged_df['Date'].isin(daily_sentiment['Date']).astype(int)\n","\n","merged_df['Sentiment_Score'] = merged_df['Sentiment_Score'].fillna(method='ffill').fillna(0)\n","merged_df['sentiment_label'] = merged_df['sentiment_label'].fillna(method='ffill').fillna(0)\n","\n","# --- ×©×œ×‘ 6: ×“×¢×™×›×” ×©×œ Is_News_Day ---\n","decay_rate = 0.005\n","decay_values = []\n","last_value = 0\n","for _, row in merged_df.iterrows():\n","    if row['Is_News_Day'] == 1:\n","        last_value = 1\n","    else:\n","        last_value = max(0, last_value - decay_rate)\n","    decay_values.append(last_value)\n","merged_df['Is_News_Day'] = decay_values\n","\n","# --- ×©×œ×‘ 7: ×™×¦×™×¨×ª score_Day ---\n","merged_df['score_Day'] = merged_df['Is_News_Day'] * merged_df['Sentiment_Score']\n","\n","# --- ×©×œ×‘ 8: ×©××™×¨×” ×œ×§×•×‘×¥ ---\n","merged_df.to_csv(output_path, index=False)\n","\n","# --- ×¡×˜×˜×•×¡ ---\n","print(\"âœ… ×˜×‘×œ×” ×¡×•×¤×™×ª ××•×›× ×” ×¢× ×›×œ ×”×¤×™×¦'×¨×™×:\")\n","print(\"ğŸ“Š ×©×•×¨×•×ª:\", len(merged_df))\n","print(\"ğŸ“… ×˜×•×•×— ×ª××¨×™×›×™×:\", merged_df['Date'].min().date(), \"â†’\", merged_df['Date'].max().date())\n","print(\"ğŸ“ × ×©××¨ ××œ:\", output_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8-FK-y2vNNa9","executionInfo":{"status":"ok","timestamp":1753735616680,"user_tz":-180,"elapsed":5792,"user":{"displayName":"Ariel.Kariv1","userId":"11324313568200724952"}},"outputId":"ad811826-93ba-458d-acf8-91eff9336343","collapsed":true},"id":"8-FK-y2vNNa9","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-83-662170529.py:51: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  news_df['overall_sentiment_label'] = news_df['overall_sentiment_label'].astype(str).str.lower().map(sentiment_map)\n","/tmp/ipython-input-83-662170529.py:53: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  news_df[sentiment_col] = pd.to_numeric(news_df[sentiment_col], errors='coerce')\n","/tmp/ipython-input-83-662170529.py:65: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  merged_df['Sentiment_Score'] = merged_df['Sentiment_Score'].fillna(method='ffill').fillna(0)\n","/tmp/ipython-input-83-662170529.py:66: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  merged_df['sentiment_label'] = merged_df['sentiment_label'].fillna(method='ffill').fillna(0)\n"]},{"output_type":"stream","name":"stdout","text":["âœ… ×˜×‘×œ×” ×¡×•×¤×™×ª ××•×›× ×” ×¢× ×›×œ ×”×¤×™×¦'×¨×™×:\n","ğŸ“Š ×©×•×¨×•×ª: 1574\n","ğŸ“… ×˜×•×•×— ×ª××¨×™×›×™×: 2018-12-10 â†’ 2025-03-14\n","ğŸ“ × ×©××¨ ××œ: /content/drive/MyDrive/Final_Project/data/processed/SNP_NEWS_FINAL.csv\n"]}]},{"cell_type":"markdown","source":["×‘×“×™×§×ª ×ª××¨×™×›×™× ×—×•×§×™×™× ×©×œ ×™××™ ××¡×—×¨"],"metadata":{"id":"HK-bXIh4SG2V"},"id":"HK-bXIh4SG2V"},{"cell_type":"code","execution_count":null,"id":"7e308966","metadata":{"id":"7e308966","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1753736027756,"user_tz":-180,"elapsed":14114,"user":{"displayName":"Ariel.Kariv1","userId":"11324313568200724952"}},"outputId":"58fe16af-ce7f-498a-f601-ba8f26903071"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… ××¡×¤×¨ ×™××™ ××¡×—×¨ ×—×•×§×™×™×: 1574\n","ğŸ“… ×ª××¨×™×š ×¨××©×•×Ÿ: 2018-12-10\n","ğŸ“… ×ª××¨×™×š ××—×¨×•×Ÿ: 2025-03-14\n"]}],"source":["import pandas as pd\n","import os\n","\n","# × ×ª×™×‘ ×œ×§×•×‘×¥ ×”××¢×•×“×›×Ÿ\n","market_path = \"/content/drive/MyDrive/Final_Project/data/processed/SNP_NEWS_FINAL.csv\"\n","\n","# ×§×¨×™××”\n","market_df = pd.read_csv(market_path)\n","\n","# × ×™×§×•×™ ××–×”××™×\n","market_df = market_df.loc[:, ~market_df.columns.str.contains('^Unnamed')]\n","market_df = market_df.drop(columns=[col for col in market_df.columns if col.strip() == \"\"])\n","\n","# ×”××¨×ª ×¢××•×“×ª Date\n","market_df['Date'] = pd.to_datetime(market_df['Date'], errors='coerce')\n","market_df = market_df[market_df['Date'].notna()].sort_values(\"Date\")\n","\n","# ×™×¦×™×¨×ª ×¨×©×™××ª ×ª××¨×™×›×™× ×—×•×§×™×™× ×‘×¤×•×¨××˜ ××—×™×“ YYYY-MM-DD\n","valid_market_dates = set(market_df[\"Date\"].dt.strftime(\"%Y-%m-%d\").tolist())\n","\n","# ×ª×¦×•×’×” ×œ×“×•×’××”\n","print(\"âœ… ××¡×¤×¨ ×™××™ ××¡×—×¨ ×—×•×§×™×™×:\", len(valid_market_dates))\n","print(\"ğŸ“… ×ª××¨×™×š ×¨××©×•×Ÿ:\", min(valid_market_dates))\n","print(\"ğŸ“… ×ª××¨×™×š ××—×¨×•×Ÿ:\", max(valid_market_dates))\n"]},{"cell_type":"markdown","source":["×”×›× ×ª ×”× ×ª×•× ×™× ×œ××•×“×œ + ×˜×¢×™× ×” ××• ××™××•×Ÿ"],"metadata":{"id":"fuZBM3samdDd"},"id":"fuZBM3samdDd"},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import joblib\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import GRU, Dense, Dropout\n","import tensorflow as tf\n","import random\n","\n","# ×§×‘×™×¢×ª seed ×œ×©×—×–×•×¨×™×•×ª\n","SEED = 42\n","np.random.seed(SEED)\n","tf.random.set_seed(SEED)\n","random.seed(SEED)\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","\n","# ×”×’×“×¨×ª × ×ª×™×‘×™×\n","BASE_DIR = \"/content/drive/MyDrive/Final_Project\"\n","DATA_PATH = os.path.join(BASE_DIR, \"data\", \"processed\", \"SNP_NEWS_FINAL.csv\")\n","MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n","os.makedirs(MODELS_DIR, exist_ok=True)\n","\n","SCALER_PATH = os.path.join(MODELS_DIR, \"final_scaler.save\")\n","MODEL_PATH = os.path.join(MODELS_DIR, \"final_snp500_model.h5\")\n","\n","# ×˜×¢×™× ×ª ×”× ×ª×•× ×™×\n","df = pd.read_csv(DATA_PATH)\n","df = df.drop(columns=[\"Date\"], errors='ignore')  # ×¢××•×“×ª ×ª××¨×™×š ×œ× × ×“×¨×©×ª ×œ××•×“×œ\n","df = df.apply(pd.to_numeric, errors='coerce').dropna()\n","features = df.columns.tolist()\n","\n","# × ×¨××•×œ\n","scaler = MinMaxScaler()\n","scaler.fit(df)\n","scaled_data = scaler.transform(df)\n","joblib.dump(scaler, SCALER_PATH)\n","\n","# ×”×’×“×¨×ª ×¨×¦×¤×™×\n","sequence_length = 30\n","forecast_horizon = 7\n","X, y = [], []\n","\n","for i in range(len(scaled_data) - sequence_length - forecast_horizon):\n","    X.append(scaled_data[i:i+sequence_length])\n","    y.append(scaled_data[i+sequence_length:i+sequence_length+forecast_horizon, features.index(\"Close\")])\n","\n","X, y = np.array(X), np.array(y)\n","\n","# ×‘× ×™×™×ª ××•×“×œ GRU\n","model = Sequential([\n","    GRU(128, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),\n","    Dropout(0.3),\n","    GRU(64),\n","    Dropout(0.3),\n","    Dense(forecast_horizon)\n","])\n","model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# ××™××•×Ÿ\n","model.fit(X, y, epochs=30, batch_size=32, verbose=1)\n","\n","# ×©××™×¨×”\n","model.save(MODEL_PATH)\n","print(\"âœ… ×”××•×“×œ ×•×”×¡×§×™×™×œ×¨ × ×©××¨×• ×‘×”×¦×œ×—×”.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Ps0RQT3ilNCs","executionInfo":{"status":"ok","timestamp":1753744447885,"user_tz":-180,"elapsed":169660,"user":{"displayName":"Ariel.Kariv1","userId":"11324313568200724952"}},"outputId":"5ed97fc5-de19-47d1-df0e-4cd810f5172e"},"id":"Ps0RQT3ilNCs","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 78ms/step - loss: 0.0910\n","Epoch 2/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 80ms/step - loss: 0.0175\n","Epoch 3/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.0123\n","Epoch 4/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - loss: 0.0108\n","Epoch 5/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - loss: 0.0087\n","Epoch 6/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 67ms/step - loss: 0.0084\n","Epoch 7/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 0.0068\n","Epoch 8/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - loss: 0.0058\n","Epoch 9/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.0052\n","Epoch 10/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 72ms/step - loss: 0.0051\n","Epoch 11/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 107ms/step - loss: 0.0049\n","Epoch 12/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 71ms/step - loss: 0.0047\n","Epoch 13/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - loss: 0.0045\n","Epoch 14/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 71ms/step - loss: 0.0045\n","Epoch 15/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 84ms/step - loss: 0.0041\n","Epoch 16/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - loss: 0.0039\n","Epoch 17/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 111ms/step - loss: 0.0034\n","Epoch 18/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - loss: 0.0033\n","Epoch 19/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - loss: 0.0031\n","Epoch 20/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 72ms/step - loss: 0.0028\n","Epoch 21/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 123ms/step - loss: 0.0032\n","Epoch 22/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 71ms/step - loss: 0.0030\n","Epoch 23/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 113ms/step - loss: 0.0026\n","Epoch 24/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 69ms/step - loss: 0.0028\n","Epoch 25/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 112ms/step - loss: 0.0026\n","Epoch 26/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 0.0024\n","Epoch 27/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - loss: 0.0025\n","Epoch 28/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - loss: 0.0022\n","Epoch 29/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 80ms/step - loss: 0.0024\n","Epoch 30/30\n","\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 72ms/step - loss: 0.0024\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["âœ… ×”××•×“×œ ×•×”×¡×§×™×™×œ×¨ × ×©××¨×• ×‘×”×¦×œ×—×”.\n"]}]},{"cell_type":"markdown","source":["×—×™×–×•×™ ×œ×ª×§×•×¤×•×ª ×©×•× ×•×ª + ×©××™×¨×” ×›Ö¾JSON"],"metadata":{"id":"cOPowaRGmfMq"},"id":"cOPowaRGmfMq"},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import json\n","import matplotlib.pyplot as plt\n","from datetime import timedelta\n","import joblib\n","from tensorflow.keras.models import load_model\n","\n","# === ×”×’×“×¨×•×ª × ×ª×™×‘×™× ===\n","BASE_DIR = \"/content/drive/MyDrive/Final_Project\"\n","DATA_PATH = os.path.join(BASE_DIR, \"data\", \"processed\", \"SNP_NEWS_FINAL.csv\")\n","SCALER_PATH = os.path.join(BASE_DIR, \"models\", \"final_scaler.save\")\n","MODEL_PATH = os.path.join(BASE_DIR, \"models\", \"final_snp500_model.h5\")\n","OUTPUT_DIR = os.path.join(BASE_DIR, \"outputs\")\n","JSON_DIR = os.path.join(OUTPUT_DIR, \"json\")\n","PLOT_DIR = os.path.join(OUTPUT_DIR, \"plots\")\n","\n","# ×™×¦×™×¨×ª ×ª×™×§×™×•×ª ×‘××™×“×ª ×”×¦×•×¨×š\n","os.makedirs(JSON_DIR, exist_ok=True)\n","os.makedirs(PLOT_DIR, exist_ok=True)\n","\n","# === ×§×¨×™××ª × ×ª×•× ×™× ===\n","df = pd.read_csv(DATA_PATH)\n","df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n","df = df.drop(columns=[col for col in df.columns if col.strip() == \"\"])\n","df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n","df = df[df['Date'].notna()].sort_values(\"Date\").reset_index(drop=True)\n","\n","# ×¨×©×™××ª ×™××™ ××¡×—×¨ ×—×•×§×™×™×\n","valid_market_dates = set(df[\"Date\"].dt.strftime(\"%Y-%m-%d\").tolist())\n","latest_real_date = df[\"Date\"].max()\n","\n","# === ×˜×¢×™× ×ª ××•×“×œ ×•×¡×§×œ×¨ ===\n","scaler = joblib.load(SCALER_PATH)\n","model = load_model(MODEL_PATH)\n","features = [col for col in df.columns if col != 'Date']\n","scaled_data = scaler.transform(df[features])\n","\n","sequence_length = 30\n","forecast_horizon = 7\n","\n","# === ×¤×•× ×§×¦×™×™×ª ×—×™×–×•×™ ===\n","def forecast_sequence(start_idx, label, future=False):\n","    input_seq = scaled_data[start_idx:start_idx + sequence_length].reshape(1, sequence_length, len(features))\n","    predicted_scaled = model.predict(input_seq)[0]\n","\n","    temp = np.zeros((forecast_horizon, len(features)))\n","    temp[:, features.index(\"Close\")] = predicted_scaled\n","    forecast_unscaled = scaler.inverse_transform(temp)[:, features.index(\"Close\")]\n","\n","    # ×ª××¨×™×›×™ ×ª×—×–×™×ª\n","    if future:\n","        forecast_dates = [latest_real_date + timedelta(days=i) for i in range(1, forecast_horizon + 1)]\n","    else:\n","        anchor_date = df[\"Date\"].iloc[start_idx + sequence_length - 1]\n","        forecast_dates = pd.date_range(end=anchor_date, periods=forecast_horizon)[::-1]\n","\n","    # ×¡×™× ×•×Ÿ ×ª××¨×™×›×™× ×—×•×§×™×™× (×œ××¢×˜ ×¢×‘×•×¨ ×—×™×–×•×™ ×¢×ª×™×“×™)\n","    filtered_dates = [d for d in forecast_dates if d.strftime(\"%Y-%m-%d\") in valid_market_dates] if not future else forecast_dates\n","\n","    if len(filtered_dates) < forecast_horizon and not future:\n","        print(f\"âš ï¸ ××–×”×¨×”: ×¨×§ {len(filtered_dates)} ×™××™× ×—×•×§×™×™× ×¢×‘×•×¨ {label}, ××š ×©×•××¨×™× ××ª ×”×§×™×™×.\")\n","    if len(filtered_dates) == 0:\n","        print(f\"âŒ ××™×Ÿ ×ª××¨×™×›×™× ×—×•×§×™×™× ×‘×›×œ×œ ×¢×‘×•×¨ {label}, ××“×œ×’×™×.\")\n","        return\n","\n","    # ×™×¦×™×¨×ª DataFrame\n","    forecast_df = pd.DataFrame({\n","        'Date': filtered_dates,\n","        'Predicted_Close': forecast_unscaled[:len(filtered_dates)]\n","    })\n","\n","    # ×—×™×©×•×‘ ××—×•×– ×©×™× ×•×™\n","    start_price = forecast_df['Predicted_Close'].iloc[0]\n","    end_price = forecast_df['Predicted_Close'].iloc[-1]\n","    change_percent = ((end_price - start_price) / start_price) * 100\n","    recommendation = \"BUY\" if change_percent > 1 else \"SELL\" if change_percent < -1 else \"HOLD\"\n","\n","    forecast_df[\"Date\"] = forecast_df[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n","\n","    # === ×©××™×¨×ª JSON ===\n","    output = {\n","        \"data\": forecast_df.to_dict(orient=\"records\"),\n","        \"recommendation\": recommendation,\n","        \"expectedChange\": f\"{change_percent:.2f}%\"\n","    }\n","    json_path = os.path.join(JSON_DIR, f\"{label}.json\")\n","    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(output, f, ensure_ascii=False, indent=2)\n","\n","    # === ×’×¨×£ PNG ===\n","    plt.figure(figsize=(10, 4))\n","    plt.plot(forecast_df[\"Date\"], forecast_df[\"Predicted_Close\"], marker=\"o\", color=\"blue\")\n","    plt.title(f\"S&P 500 Forecast â€“ {label}\")\n","    plt.xlabel(\"Date\")\n","    plt.ylabel(\"Predicted Close Price\")\n","    plt.grid(True)\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","    plot_path = os.path.join(PLOT_DIR, f\"{label}.png\")\n","    plt.savefig(plot_path)\n","    plt.close()\n","\n","    print(f\"âœ… {label} â€“ ×©××•×¨ ({recommendation} / {change_percent:.2f}%)\")\n","\n","# === ×”×¨×¦×•×ª ×—×™×–×•×™ ===\n","\n","# 1. ×ª×—×–×™×ª ×¢×ª×™×“×™×ª (×©×‘×•×¢ ×§×“×™××”)\n","forecast_sequence(start_idx=len(df) - sequence_length, label=\"forecast_latest\", future=True)\n","\n","# 2. ×ª×—×–×™×•×ª ×”×™×¡×˜×•×¨×™×•×ª\n","dates_back = {\n","    \"forecast_week1\": \"2025-03-14\",\n","    \"forecast_week2\": \"2025-03-07\",\n","    \"forecast_week3\": \"2025-03-01\"\n","}\n","\n","for label, date_str in dates_back.items():\n","    target_date = pd.to_datetime(date_str)\n","    possible_dates = df[df[\"Date\"] <= target_date]\n","    if not possible_dates.empty:\n","        end_idx = possible_dates.index[-1]\n","        if end_idx >= sequence_length:\n","            forecast_sequence(start_idx=end_idx - sequence_length, label=label)\n","        else:\n","            print(f\"âŒ ×œ× ××¡×¤×™×§ × ×ª×•× ×™× ×¢×‘×•×¨ {label}\")\n","    else:\n","        print(f\"âŒ ×œ× × ××¦× ×ª××¨×™×š ×—×•×§×™ ×¢×‘×•×¨ {label}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ezB82XzbmidN","executionInfo":{"status":"ok","timestamp":1753746082511,"user_tz":-180,"elapsed":9320,"user":{"displayName":"Ariel.Kariv1","userId":"11324313568200724952"}},"outputId":"8354d63d-4d52-497d-e3f0-895576e77ebc","collapsed":true},"id":"ezB82XzbmidN","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","âœ… forecast_latest â€“ ×©××•×¨ (BUY / 1.03%)\n","\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n","âš ï¸ ××–×”×¨×”: ×¨×§ 5 ×™××™× ×—×•×§×™×™× ×¢×‘×•×¨ forecast_week1, ××š ×©×•××¨×™× ××ª ×”×§×™×™×.\n","âœ… forecast_week1 â€“ ×©××•×¨ (HOLD / -0.25%)\n","\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n","âš ï¸ ××–×”×¨×”: ×¨×§ 5 ×™××™× ×—×•×§×™×™× ×¢×‘×•×¨ forecast_week2, ××š ×©×•××¨×™× ××ª ×”×§×™×™×.\n","âœ… forecast_week2 â€“ ×©××•×¨ (HOLD / -0.28%)\n","\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n","âš ï¸ ××–×”×¨×”: ×¨×§ 5 ×™××™× ×—×•×§×™×™× ×¢×‘×•×¨ forecast_week3, ××š ×©×•××¨×™× ××ª ×”×§×™×™×.\n","âœ… forecast_week3 â€“ ×©××•×¨ (HOLD / -0.18%)\n"]}]},{"cell_type":"markdown","source":["×¡×™× ×•×Ÿ ×”×—×™×–×•×™×™× ×œ×™××™ ×”××¡×—×¨ ×‘×¤×•×¢×œ + ×™×¦×•×¨ ×§×•×‘×¥ ×ª×—×–×™×•×ª ×××•×—×“"],"metadata":{"id":"SKcYveSZyI3b"},"id":"SKcYveSZyI3b"},{"cell_type":"code","source":["import json\n","import os\n","from datetime import datetime\n","\n","# × ×ª×™×‘ ×œ×§×‘×¦×™× ×”×—×“×©×™× ×‘×ª×•×š outputs/json\n","json_dir = \"/content/drive/MyDrive/Final_Project/outputs/json\"\n","\n","forecast_files = [\n","    os.path.join(json_dir, \"forecast_latest.json\"),\n","    os.path.join(json_dir, \"forecast_week1.json\"),\n","    os.path.join(json_dir, \"forecast_week2.json\"),\n","    os.path.join(json_dir, \"forecast_week3.json\")\n","]\n","\n","# ×˜×•×¢×Ÿ ××ª ×›×•×œ×\n","forecast_all = []\n","\n","for file in forecast_files:\n","    if os.path.exists(file):\n","        with open(file, encoding='utf-8') as f:\n","            data = json.load(f)\n","            forecast_all.extend(data[\"data\"])  # ××ª×•×š ××¤×ª×— \"data\"\n","    else:\n","        print(f\"âš ï¸ ×§×•×‘×¥ ×œ× × ××¦×: {file}\")\n","\n","# ×”×¡×¨×” ×©×œ ×›×¤×™×œ×•×™×•×ª ×•××™×•×Ÿ ×œ×¤×™ ×ª××¨×™×š\n","forecast_all = {entry['Date']: entry for entry in forecast_all}  # ××¡×™×¨ ×›×¤×•×œ×™× ×œ×¤×™ ×ª××¨×™×š\n","forecast_all = list(forecast_all.values())\n","forecast_all.sort(key=lambda x: x['Date'])\n","\n","# ×©××™×¨×” ×œ×§×•×‘×¥ ×—×“×©\n","output_path = os.path.join(json_dir, \"forecast_all.json\")\n","with open(output_path, \"w\", encoding='utf-8') as f:\n","    json.dump(forecast_all, f, indent=2, ensure_ascii=False)\n","\n","print(f\"âœ”ï¸ × ×•×¦×¨ ×§×•×‘×¥ forecast_all.json ×¢× {len(forecast_all)} ×ª××¨×™×›×™×.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"moshrSS9yL5g","executionInfo":{"status":"ok","timestamp":1753746813832,"user_tz":-180,"elapsed":57,"user":{"displayName":"Ariel.Kariv1","userId":"11324313568200724952"}},"outputId":"3fa87abd-f300-430c-bc42-604320620f1a"},"id":"moshrSS9yL5g","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ”ï¸ × ×•×¦×¨ ×§×•×‘×¥ forecast_all.json ×¢× 22 ×ª××¨×™×›×™×.\n"]}]},{"cell_type":"markdown","source":["×©××™×¨×ª × ×ª×•× ×™ ×©×•×§ ×××™×ª×™×™×"],"metadata":{"id":"YHaRMV3mnRse"},"id":"YHaRMV3mnRse"},{"cell_type":"code","source":["# ×©××™×¨×ª ××—×™×¨×™ ×”×¡×’×™×¨×” ×‘×¤×•×¢×œ ×œ×—×•×“×© ×”××—×¨×•×Ÿ\n","start_date = pd.to_datetime(\"2025-02-14\")\n","end_date = pd.to_datetime(\"2025-03-14\")\n","\n","actual_df = market_df.copy()\n","actual_df = actual_df[(actual_df['Date'] >= start_date) & (actual_df['Date'] <= end_date)]\n","actual_json = actual_df[['Date', 'Close']].copy()\n","actual_json['Date'] = actual_json['Date'].dt.strftime('%Y-%m-%d')\n","\n","output_path = os.path.join(OUTPUT_JSON, \"actual_data.json\")\n","actual_json.to_json(output_path, orient=\"records\", indent=2)\n","\n","print(f\"âœ… actual_data.json × ×©××¨ ({len(actual_json)} ×ª××¨×™×›×™×)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zLGIXzWgnSPn","executionInfo":{"status":"ok","timestamp":1753747907520,"user_tz":-180,"elapsed":28,"user":{"displayName":"Ariel.Kariv1","userId":"11324313568200724952"}},"outputId":"9e0d2b58-70b5-4d40-8953-725f5160cb54"},"id":"zLGIXzWgnSPn","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… actual_data.json × ×©××¨ (20 ×ª××¨×™×›×™×)\n"]}]},{"cell_type":"markdown","source":["×©××™×¨×ª ×—×“×©×•×ª ×›×œ×›×œ×™×•×ª"],"metadata":{"id":"D5p-ef2qnVBn"},"id":"D5p-ef2qnVBn"},{"cell_type":"code","source":["import pandas as pd\n","import os\n","import json\n","\n","# × ×ª×™×‘ ×§×•×‘×¥ × ×ª×•× ×™ ×—×“×©×•×ª\n","RAW_NEWS_PATH = \"/content/drive/MyDrive/Final_Project/data/raw/economic_news_cleaned.csv\"\n","OUTPUT_JSON = \"/content/drive/MyDrive/Final_Project/outputs/json\"\n","\n","# ×§×¨×™××”\n","news_df = pd.read_csv(RAW_NEWS_PATH)\n","news_df = news_df.loc[:, ~news_df.columns.str.contains('^Unnamed')]\n","news_df = news_df.drop(columns=[col for col in news_df.columns if col.strip() == \"\"])\n","news_df['Date'] = pd.to_datetime(news_df['Date'], errors='coerce', dayfirst=True)\n","news_df = news_df[news_df['Date'].notna()]\n","\n","# ×¢×™×‘×•×“\n","news_json = news_df[['Date', 'Sentiment_Label', 'summary', 'source', 'title']].copy()\n","news_json['Date'] = news_json['Date'].dt.strftime('%Y-%m-%d')\n","news_json.columns = ['Date', 'Sentiment_Label', 'summary', 'Source', 'title']\n","\n","# ×©××™×¨×”\n","os.makedirs(OUTPUT_JSON, exist_ok=True)\n","output_path = os.path.join(OUTPUT_JSON, \"news_data.json\")\n","news_json.to_json(output_path, orient=\"records\", indent=2, force_ascii=False)\n","\n","print(f\"âœ… news_data.json × ×©××¨ ({len(news_json)} ×™×“×™×¢×•×ª)\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UkYB34YQnXSR","executionInfo":{"status":"ok","timestamp":1753748630971,"user_tz":-180,"elapsed":1689,"user":{"displayName":"Ariel.Kariv1","userId":"11324313568200724952"}},"outputId":"919cfb36-14eb-497a-c929-98b9e2cd6391"},"id":"UkYB34YQnXSR","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… news_data.json × ×©××¨ (36625 ×™×“×™×¢×•×ª)\n"]}]},{"cell_type":"markdown","source":["××™×™×¦×¨ ××ª market_dashboard_data.json"],"metadata":{"id":"Myxi7j9PuHne"},"id":"Myxi7j9PuHne"},{"cell_type":"code","source":["import pandas as pd\n","import os\n","import json\n","\n","# ×§×œ×˜ ×•×¤×œ×˜\n","RAW = \"/content/drive/MyDrive/Final_Project/data/processed/SNP_NEWS_FINAL.csv\"\n","OUT = \"/content/drive/MyDrive/Final_Project/outputs/json/market_dashboard_data.json\"\n","\n","# ×§×¨×™××”\n","df = pd.read_csv(RAW)\n","df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n","df = df.drop(columns=[col for col in df.columns if col.strip() == \"\"])\n","df['Date'] = pd.to_datetime(df['Date'], errors='coerce', dayfirst=True)\n","df = df[df['Date'].notna()].sort_values(\"Date\").reset_index(drop=True)\n","\n","# ×¢×™×‘×•×“\n","df = df[[\"Date\", \"Close\", \"Volume\"]].copy()\n","df[\"MA20\"] = df[\"Close\"].rolling(window=20).mean()\n","\n","# RSI\n","delta = df[\"Close\"].diff()\n","gain = delta.where(delta > 0, 0).rolling(window=14).mean()\n","loss = -delta.where(delta < 0, 0).rolling(window=14).mean()\n","rs = gain / loss\n","df[\"RSI\"] = 100 - (100 / (1 + rs))\n","\n","# MACD\n","ema12 = df[\"Close\"].ewm(span=12, adjust=False).mean()\n","ema26 = df[\"Close\"].ewm(span=26, adjust=False).mean()\n","df[\"MACD\"] = ema12 - ema26\n","\n","# ×”××œ×¦×”\n","def reco(row):\n","    if row[\"RSI\"] < 30 and row[\"MACD\"] > 0:\n","        return \"BUY\"\n","    elif row[\"RSI\"] > 70 and row[\"MACD\"] < 0:\n","        return \"SELL\"\n","    else:\n","        return \"HOLD\"\n","\n","df[\"Recommendation\"] = df.apply(reco, axis=1)\n","\n","# ×©××™×¨×” ×œÖ¾JSON\n","result = []\n","for _, row in df.dropna().iterrows():\n","    result.append({\n","        \"date\": row[\"Date\"].strftime(\"%Y-%m-%d\"),\n","        \"rsi\": round(row[\"RSI\"], 2),\n","        \"macd\": round(row[\"MACD\"], 2),\n","        \"volatility\": int(row[\"Volume\"]),\n","        \"MA20\": round(row[\"MA20\"], 2),\n","        \"Recommendation\": row[\"Recommendation\"]\n","    })\n","\n","with open(OUT, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(result, f, indent=2, ensure_ascii=False)\n","\n","print(\"âœ… market_dashboard_data.json × ×•×¦×¨ ×•× ×©××¨!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nQVrAI3luNrI","executionInfo":{"status":"ok","timestamp":1753748725700,"user_tz":-180,"elapsed":247,"user":{"displayName":"Ariel.Kariv1","userId":"11324313568200724952"}},"outputId":"4071713a-74bc-4c4c-f480-dc75377d0f61"},"id":"nQVrAI3luNrI","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… market_dashboard_data.json × ×•×¦×¨ ×•× ×©××¨!\n"]}]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}